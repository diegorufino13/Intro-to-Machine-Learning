# -*- coding: utf-8 -*-
"""HW1 IntroToML .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11PZaHGRitNiHIzJ5ptURDGrY9rGa84XX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Intro to ML/D3.csv'
sample = pd.DataFrame(pd.read_csv(file_path))
sample.head()

df = sample

# Separate features and labels
X1 = df.values[:, 0]  # get input values from x1 column
X2 = df.values[:, 1]  # get input values from x2 column
X3 = df.values[:, 2]  # get input values from x3 column
Y = df.values[:, 3]  # get output values from y column
m = len(Y)  # Number of training examples
n1 = len(X1)  # Number of training examples
n2 = len(X2)
n3 = len(X3)

# Display first 5 records and the total number of training examples
print('X1 = ', X1[: 5])
print('X2 = ', X2[: 5])
print('X3 = ', X3[: 5])
print('Y = ', Y[: 5])
print('m = ', m)
print('n1 = ', n1)
print('n2 = ', n2)
print('n3 = ', n3)

print('X1 = ', X1[: 100])
print('X2 = ', X2[: 100])
print('X3 = ', X3[: 100])
print('Y = ', Y[: 100])

from IPython.display import display
display(df)

"""X1 Variable"""

X1 = df.values[:, 0]  # get input values from x1 column
Y = df.values[:, 3]  # get output values from y column

# Scatter plot for X1
plt.scatter(X1, Y, color = 'blue', marker='+')


# Grid, labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X1 Variable')
plt.ylabel('Output')
plt.title('Scatter plot of training data based on X1 variable')

# Show the plot
plt.show()

"""X2 Variable"""

X2 = df.values[:, 1]  # get input values from x2 column
Y = df.values[:, 3]  # get output values from y column

# Scatter plot for X2
plt.scatter(X2, Y, color = 'blue', marker='+')

# Grid, labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X2 Variable')
plt.ylabel('Output')
plt.title('Scatter plot of training data based on X2 variable')

# Show the plot
plt.show()

"""X3 Variable"""

X3 = df.values[:, 2]  # get input values from x3 column
Y = df.values[:, 3]  # get output values from y column

#Scatter plot for X3
plt.scatter(X3, Y, color = 'blue', marker='+')

# Grid, labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X3 Variable')
plt.ylabel('Output')
plt.title('Scatter plot of training data based on X3 variable')

# Show the plot
plt.show()

X_0 = np.ones((m, 1))
X_0[:5]

X_1 = X1.reshape(m, 1)
X_1[:10]

X_2 = X2.reshape(m, 1)
X_2[:10]

X_3 = X3.reshape(m, 1)
X_3[:10]

Xtotal1 = np.hstack((X_0, X_1))
Xtotal1[:5]

Xtotal2 = np.hstack((X_0, X_2))
Xtotal2[:5]

Xtotal3 = np.hstack((X_0, X_3))
Xtotal3[:5]

theta = np.zeros(2)
theta

def compute_cost( X, Y, theta):
    """
    Compute cost for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represent the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)

    Returns:
    J : Scalar value, the cost
    """
    predictions = X.dot(theta)
    errors = np.subtract(predictions, Y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

# Lets compute the cost for theta values
cost1 = compute_cost(Xtotal1, Y, theta)
print('The cost for given values of Input X1 and Output Y =', cost1)

# Lets compute the cost for theta values
cost2 = compute_cost(Xtotal2, Y, theta)
print('The cost for given values of Input X2 and Output Y =', cost2)

# Lets compute the cost for theta values
cost3 = compute_cost(Xtotal3, Y, theta)
print('The cost for given values of Input X3 and Output Y =', cost3)

def gradient_descent(X, Y, theta, alpha, iterations):
    """
    Compute the optimal parameters using gradient descent for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represents the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)
    alpha : Learning rate (scalar)
    iterations : Number of iterations (scalar)

    Returns:
    theta : Updated values of fitting parameters or weights after 'iterations' iterations. Dimension (n)
    cost_history : Array containing the cost for each iteration. Dimension (iterations)
    """

    m = len(Y)  # Number of training examples
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, Y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, Y, theta)

    return theta, cost_history

theta = [0., 0.]
iterations = 1500
alpha = 0.01

theta, cost_history = gradient_descent(Xtotal1, Y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history of X1=', cost_history)

# Assuming that X, y, and theta are already defined
# Also assuming that X has two columns: a feature column and a column of ones

# Scatter plot for the training data
plt.scatter(Xtotal1[:, 1], Y, color='red', marker='+', label='Training Data')

# Line plot for the linear regression model
plt.plot(Xtotal1[:, 1], Xtotal1.dot(theta), color='green', label='Linear Regression')

# Plot customizations
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X1 Variable')
plt.ylabel('Output')
plt.title('Linear Regression Fit for X1')
plt.legend()

# Show the plot
plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent for X1')

# Show the plot
plt.show()

theta, cost_history = gradient_descent(Xtotal2, Y, theta, 0.01, 1500)
print('Final value of theta =', theta)
print('cost_history of X2=', cost_history)

# Assuming that X, y, and theta are already defined
# Also assuming that X has two columns: a feature column and a column of ones

# Scatter plot for the training data
plt.scatter(Xtotal2[:, 1], Y, color='red', marker='+', label='Training Data')

# Line plot for the linear regression model
plt.plot(Xtotal2[:, 1], Xtotal2.dot(theta), color='green', label='Linear Regression')

# Plot customizations
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X2 Variable')
plt.ylabel('Output')
plt.title('Linear Regression Fit of X2')
plt.legend()

# Show the plot
plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent for X2')

# Show the plot
plt.show()

theta, cost_history = gradient_descent(Xtotal3, Y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history of X1=', cost_history)

# Assuming that X, y, and theta are already defined
# Also assuming that X has two columns: a feature column and a column of ones

# Scatter plot for the training data
plt.scatter(Xtotal3[:, 1], Y, color='red', marker='+', label='Training Data')

# Line plot for the linear regression model
plt.plot(Xtotal3[:, 1], Xtotal3.dot(theta), color='green', label='Linear Regression')

# Plot customizations
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X3 Variable')
plt.ylabel('Output')
plt.title('Linear Regression Fit of X3')
plt.legend()

# Show the plot
plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent for X3')

# Show the plot
plt.show()

"""#Number 2"""

XtotalOfAll = np.hstack((X_0, X_1,X_2,X_3))
XtotalOfAll[:5]

theta = np.zeros(4)
theta

def compute_cost( X, Y, theta):
    """
    Compute cost for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represent the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)

    Returns:
    J : Scalar value, the cost
    """
    predictions = X.dot(theta)
    errors = np.subtract(predictions, Y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

iterations = 450
alpha = 0.01

# Lets compute the cost for theta values
costOfAll = compute_cost(XtotalOfAll, Y, theta)
print('The cost for given values of all inputs and Output Y =', costOfAll)

def gradient_descent(X, Y, theta, alpha, iterations):
    """
    Compute the optimal parameters using gradient descent for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represents the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)
    alpha : Learning rate (scalar)
    iterations : Number of iterations (scalar)

    Returns:
    theta : Updated values of fitting parameters or weights after 'iterations' iterations. Dimension (n)
    cost_history : Array containing the cost for each iteration. Dimension (iterations)
    """

    m = len(Y)  # Number of training examples
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, Y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, Y, theta)

    return theta, cost_history

theta, cost_history = gradient_descent(XtotalOfAll, Y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history of XtotalOfAll=', cost_history)

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent for XtotalOfAll')

# Show the plot
plt.show()

#Y Values

y1= theta[0] + theta[1]*(1) + theta[2]*(1)+ theta[3]*(1)
print('y value given (1,1,1)', y1)

y2= theta[0] + theta[1]*(2) + theta[2]*(0)+ theta[3]*(4)
print('y value given (2,0,4)', y2)

y1= theta[0] + theta[1]*(3) + theta[2]*(2)+ theta[3]*(1)
print('y value given (3,2,1)', y1)
